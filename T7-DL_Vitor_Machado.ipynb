{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importações de biblilotecas"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploração dos Dados"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data= pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = train_data.corr()\ncorrelation_matrix['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(correlation_matrix['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como o menor valor para a correlação com o preço de venda da casa tem um módulo baixo, podemos nos preocupar apenas com os que possuem valores positivos. Para filtrar quais são os mais importantes, vamos considerar os que possuem correlação maior do que 0.5 com o preço de venda."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = []\nfor i in range(len(correlation_matrix['SalePrice'])):\n    if correlation_matrix['SalePrice'][i] > 0.5:\n        labels.append(correlation_matrix['SalePrice'].index[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_data = train_data[labels]\n\nnew_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Todos são valores numéricos, o que já facilita bastante nosso problema! Mas ainda podemos tratar melhor esses dados. Primeiro, vamos procurar por outliers. Para isso, vamos usar um boxplot para cada parâmetro."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in new_train_data.columns:\n    sns.boxplot(new_train_data[i])\n    plt.title(f'{i}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Procurando valores faltantes\n\nnew_train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com exceção de 2 parâmetros, todos os demais apresentam outliers. Importante observar que, provavelmente, alguns deles se referem ao mesmo dado. Vamos remover todos para termos dados mais suaves para o modelo treinar."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_zscores = zscore(new_train_data)\nabs_zscores = np.abs(train_zscores)\nzscore_filter = (abs_zscores<3).all(axis=1)\n\nnew_train_data=new_train_data[zscore_filter]\nlen(new_train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Foram eliminados apenas 54 amostras, parece uma perda razoável. Agora, podemos normalizar os dados e passar para o treinamento dos modelos. Não podemos esquecer de tirar o valor valor de venda da casa antes de normalizar os dados!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = new_train_data.pop('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = new_train_data.values\nmin_max = MinMaxScaler()\nx_scaled = min_max.fit_transform(x)\nx_train = pd.DataFrame(x_scaled)\n\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestRegressor(random_state=0)\nforest.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Feature com maior importância de acordo com o modelo: {labels[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A floresta conseguiu descrever bem os dados de treino e a featura a qual ela deu maior importância é a mesma que possui maior correlação com o preço, de acordo com nossa matriz de correlação.\n\nAgora precisamos separar as features importantes nos dados de teste e colocar a floresta para fazer uma previsão."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.pop(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test_data[labels]\nx_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Valores faltantes para garagem e e porão podem indicar que essas casas simplesmente não possuem esses cômodos. Por isso, vamos preencher seus valores com 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.fillna(0)\nx_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x_test.values\nmin_max = MinMaxScaler()\nx_scaled = min_max.fit_transform(x)\nx_test = pd.DataFrame(x_scaled)\n\nx_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = forest.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora vamos submeter as predições e ver nosso resultado"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'Id': x_test.index+1461,\n                       'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score obtido: 0.34482"},{"metadata":{},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light_regressor = lgb.LGBMRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_pretest, y_train, y_pretest = train_test_split(x_train, y_train, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light_regressor.fit(x_train, y_train, eval_set=[(x_pretest, y_pretest)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light_pred = light_regressor.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_light = pd.DataFrame({'Id': x_test.index+1461,\n                       'SalePrice': light_pred})\noutput_light.to_csv('light_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_light","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score: 0.33760"},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nlight_pred_train = light_regressor.predict(x_train)\nforest_pred_train = forest.predict(x_train)\n\nlight_error = np.sqrt(mean_squared_error(light_pred_train, y_train))\nforest_error = np.sqrt(mean_squared_error(forest_pred_train, y_train))\n\nprint(light_error)\nprint(forest_error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame({'forest': forest_pred_train, 'light':light_pred_train})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Os pesos foram dados de forma a dar mais valor ao modelo com menor loss.\n\nforest_weight = 1 - forest_error/(forest_error+light_error)\nlight_weight = 1 - forest_weight\n\npred_df['ensemble'] = forest_weight*pred_df['forest'] + light_weight*pred_df['light']\n\nprint(np.sqrt(mean_squared_error(pred_df['ensemble'], y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}